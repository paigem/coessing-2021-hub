{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Very brief introduction to the python libraries: `xarray` and `dask`\n",
    "\n",
    "- `xarray` is similar to the `pandas` library, but for multi-dimensional data. Whereas `pandas` works well with 2-d or tabular data, it is common in oceanography (at least in physical oceanography) to have 3-d or 4-d data (with 3 spatial dimensions and 1 time dimension). `xarray` is also very useful for analyzing large volumes of data often in netcdf (.nc) format.\n",
    "- `dask` is a library that parallelizes code (i.e. can run multiple computations at the same time) in a relatively easy and efficient way. It is very useful when using big data and integrates nicely with `xarray`.\n",
    "\n",
    "This notebook is meant to only be a *very brief* introduction to these two libraries, so you know where to start if you'd like to use these tools in your own research/coding. There are a few very nice tutorials online about both of these libraries, including these two by Ryan Abernathey:\n",
    "\n",
    "- \"Xarray Fundamentals\": https://rabernat.github.io/research_computing_2018/xarray.html \n",
    "- \"Dask for Parallel Computing and Big Data\": https://rabernat.github.io/research_computing_2018/dask-for-parallel-computing-and-big-data.html#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a dataset\n",
    "\n",
    "This is a dataset from 41 years of the CESM (Community Earth System Model) ocean model (\"POP\": Parallel Ocean Program). This dataset is stored on the Cloud through the Pangeo Project, and we can load it directly into this notebook that is running on the Cloud too! \n",
    "\n",
    "When we say \"load\" here, we actually don't load the entire dataset into memory. Instead, we do what is called \"lazy loading\", where we load only the dataset metadata initially. In this way, we can examine the dataset (which variables, size of variables, names of dimensions, etc.) without having to worry about how large the dataset is. This is very handy when using large datasets (such as the CESM dataset we will use here which is a whopping 5.6  terabytes!), since the dataset is too large to fit into any computer's memory. \n",
    "\n",
    "But, often we are only interested in one variable (say, sea surface temperature) or a specific spatial region or time frame, and so we only need a small subset of the larger dataset. `xarray` and `dask` allow us to work with large datasets and only load in the portion of the data that we actually are interested in, making it possible to do data analysis on very large datasets.\n",
    "\n",
    "We load in the (meta)data below using the library `intake`, which (among other features) provides a quick and standardized way to load data from a catalog. You can read more about `intake` [here](https://intake.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intake import open_catalog\n",
    "\n",
    "cat = open_catalog(\"https://raw.githubusercontent.com/pangeo-data/pangeo-datastore/master/intake-catalogs/ocean/CESM_POP.yaml\")\n",
    "ds  = cat[\"CESM_POP_hires_control\"].to_dask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To view details of the dataset, simply type its name (we called it `ds` above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So you can see that the data has a few things to note:\n",
    "\n",
    "- the data is a `xarray.Dataset` type\n",
    "- 8 dimensions of different sizes: nlat, nlon, time, ...\n",
    "- 59 coordinates: time, along with many others\n",
    "- 12 variables: SST, and many others\n",
    "- a list of attributes (i.e. metadata about the dataset: full name of model run, time format, etc.)\n",
    "\n",
    "The above output is actually interactive, so you can click on the arrows to the lft of the words (e.g. the arrow to the left of \"Data variables\" to view a list of variables stored in this dataset. You can also click on the icons on the right end of each line. So, if you want to know what the variable SST is, click the paper icon on the right and it will tell you that it is the \"Surface Potential Temperature\" in degrees Celsius (\"degC\").\n",
    "\n",
    "The icon that looks like stacked cylinders immediately to the right of the paper icon is also clickable, and shows information about the size, shape, and type of data in that variable. Notice that it has two columns: one for the full variable array and one for single chunks of the DataArray. As you can see, the chunks are much smaller in size than the full array, and chunking up arrays into small, manageable sizes is one of the main ways we can reduce computations on large datasets into doable actions.\n",
    "\n",
    "You can start to understand just how useful xarray, as it can load and display all different types of information about a dataset.\n",
    "\n",
    "To access items within the dataset, we just type `ds.` followed by the aspect of the dataset we are interested in. The following few cells show a few examples.\n",
    "\n",
    "See information about the variable 'SST':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.SST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that SST is an `xarray.DataArray`, as opposed to a `xarray.Dataset` like our `ds` above. In `xarray`, a Dataset is a collection of DataArrays. So typically, each variable and coordinate is a DataArray, and all together they make up a Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List information about the time dimension. This model uses `cftime` - a python library for denoting time - and uses a calendar without leap years. Hence the `cftime.DateTimeNoLeap` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the first three timesteps start with the number 46. In this model, 46 is the starting model year, while the numbers following are the month and day, respectively. So this dataset starts on January 2nd, in the 46th year of the model run, and ends on January 1st, year 87 of the model. So this tells us that we have ~41 years of data, at daily output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a few dimensions in a global ocean model like this. Here, `nlat` and `nlon` are the number of points in the latitudinal and longitudinal directions (i.e. not the lat/lon values themselves)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the great features of xarray is that it is aware of the names of dimensions. So if, for instance, you'd like to grab the output from only the first day of the dataset, you can do so as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the SST output from the first day only \n",
    "ds.SST.isel(time=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we don't need to specify which axis number `time` is - we need only write 'time' and xarray knows which dimension that is. The `.isel()` functions selects the specified range (in the above case, the 0th index of `time`). We can also grab a larger range of values, e.g. by grabbing the first 10 days of SST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the SST output from the first day only \n",
    "ds.SST.isel(time=slice(0,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xarray also allows for some simple data manipulation, such as taking the mean of a dataset. First we select the SST variable, and then we write out the function `.mean()` with the argument 'time' to take the time average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.SST.mean('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we just took the mean of a dataset in one line! What if we want to plot some data? Let's try to plot the first day of SST data. All you have to do is add `.plot()` to the end of the line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this may take several seconds\n",
    "ds.SST.isel(time=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that this sook several seconds. Until now, we hadn't actually *loaded* any of the data. We had just loaded metadata - enough for us to look at the data and understand it a bit better. But, to make a plot, we need to actually access the data, and so adding `.plot()` triggers a load of the data, but only what is needed to make the plot. Since we only plotted the first day, it was reasonably quick. \n",
    "\n",
    "Side note: This plot can look much spiffier with the python library `cartopy`! If you want to try out `cartopy`, go through to the cartopy tutorial that Josu√© made for you and attend his tutorial!\n",
    "\n",
    "#### Ok, that is all we will do with xarray for now. I hope you can see how useful it can be, and you will be making use of xarray in the ECCO tutorials!\n",
    "\n",
    "You can also check out the `xarray` tutorial linked at the top of this notebook - it goes into much more detail!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask\n",
    "\n",
    "## In this brief dask tutorial (largely based on [this one](https://tutorial.dask.org/00_overview.html)), we will see how dask can help speed up your computations.\n",
    "\n",
    "### Dask has two parts:\n",
    "1. distributed computing for running code in parallel\n",
    "2. helping with large-than-memory datasets and lazy loading/computations - this works with libraries like `xarray`, as well as `pandas`\n",
    "\n",
    "**1. Parallel computing**\n",
    "\n",
    "We will start by defining some very basic adding and multiplying functions that use the function `sleep()` from the `time` library. This sleep() function causes the function add() that we define below to pause for the number of seconds inside the parentheses.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "# Define a function to add two numbers\n",
    "def add(x,y):\n",
    "    sleep(2) # pause for 2 seconds\n",
    "    return x+y\n",
    "\n",
    "# Define a function to multiply two numbers\n",
    "def mlt(x,y):\n",
    "    sleep(1) # pause for 1 second\n",
    "    return x*y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we will use another 'magic' function similar to the `%matplotlib inline` function you have probably used numerous times. These function calls that begin with a `%` are called 'magic' functions in ipython notebooks. This time we will use `%%time`, which prints out the amount of time it takes to run all of the code in that particular cell.\n",
    "\n",
    "If we call add() once and mlt() twice, can you guess how long it will take to run? It should take almost exactly 2 + 1 + 1 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "a = add(1,2)\n",
    "b = mlt(1,2)\n",
    "c = mlt(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my JupyterHub instance, it took 4.0 seconds to run - exactly as expected! \n",
    "\n",
    "But, we could theoretically run all of these functions at the same time, since they are all independent calculations. `dask` can help us do that! We are now going to import a dask function called 'delayed'. It is so called because it doesn't run a function immediately, but stores the information to run a function until the user specifies a `.compute()` function, at which point the calculation is run, and in the most optimized way. Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "a = delayed(add)(1,2)\n",
    "b = delayed(mlt)(1,2)\n",
    "c = delayed(mlt)(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, this claims that the calculations ran in 358 microseconds - but that doesn't make sense! Our two functions require at least a 1 second pause when run. The catch is that we haven't actually done the calculation yet. We have just created delayed objects that will run once we compute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "c.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it looks like it only took 3.07 seconds to run the same calculation as before when it took 4 seconds! While 1 second doesn't sound like much, this can be scaled up with large amounts of data. You can save hours or even days or your time by parallelizing with dask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Dask with big data**\n",
    "\n",
    "Now we will briefly look at the second aspect of Dask. Take a look at our CESM dataset that we loaded during the xarray portion of this tutorial. Let's view it again here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the arrow to the left of the \"Data variables\", and you should see the list of variables appear. If you read each variable line, you will see that it says `dask.array<chunksize=(1, 2400, 3600)`. What this tells us is that each `xarray.DataArray` actually wraps a `dask.array`. So xarray and dask are intertwined, and work together to make data analysis of large datasets possible. \n",
    "\n",
    "Check out the ECCO notebooks and tutorials for more practice using `xarray` and `dask`, with some real examples of analyzing a global ocean dataset!\n",
    "\n",
    "If you want to learn more about Dask, go through the tutorial linked at the top of the page.\n",
    "\n",
    "Note that dask may not be necessary to use if you do not have big data! If you do have large amounts of data that take a long time to run, then dask is a great resource, and you can use it on your local computer, on high performance computing infrastructures, or on the cloud!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
